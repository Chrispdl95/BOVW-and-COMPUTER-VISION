{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48669683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans #KMeans\n",
    "import numpy as np\n",
    "#import secondary functions that will be used very frequent\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f81e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a dictionary that holds all images category by category.\n",
    "def load_images_from_folder(folder, inputImageSize ):\n",
    "    images = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        category = []\n",
    "        path = folder + \"/\" + filename\n",
    "        for cat in os.listdir(path):\n",
    "            img = cv2.imread(path + \"/\" + cat)\n",
    "            #print(' .. parsing image', cat)\n",
    "            if img is not None:\n",
    "                # grayscale it\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                #resize it, if necessary\n",
    "                img = cv2.resize(img, (inputImageSize[0], inputImageSize[1]))\n",
    "\n",
    "                category.append(img)\n",
    "        images[filename] = category\n",
    "        print(' . Finished parsing images. What is next?')\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61a12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates descriptors using an approach of your choise. e.g. ORB, SIFT, SURF, FREAK, MOPS, ετψ\n",
    "# Takes one parameter that is images dictionary\n",
    "# Return an array whose first index holds the decriptor_list without an order\n",
    "# And the second index holds the sift_vectors dictionary which holds the descriptors but this is seperated class by class\n",
    "def detector_features(images):\n",
    "    print(' . start detecting points and calculating features for a given image set')\n",
    "    detector_vectors = {}\n",
    "    descriptor_list = []\n",
    "    #sift = cv2.xfeatures2d.SIFT_create()\n",
    "    detectorToUse = cv2.xfeatures2d.SIFT_create()\n",
    "    #detectorToUse = cv2.ORB_create()\n",
    "    for nameOfCategory, availableImages in images.items():\n",
    "        features = []\n",
    "        for img in availableImages: # reminder: val\n",
    "            kp, des = detectorToUse.detectAndCompute(img, None)\n",
    "\n",
    "            descriptor_list.extend(des)\n",
    "            features.append(des)\n",
    "        detector_vectors[nameOfCategory] = features\n",
    "        print(' . finished detecting points and calculating features for a given image set')\n",
    "    return [descriptor_list, detector_vectors] # be aware of the []! this is ONE output as a list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba817643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A k-means clustering algorithm who takes 2 parameter which is number\n",
    "# of cluster(k) and the other is descriptors list(unordered 1d array)\n",
    "# Returns an array that holds central points.\n",
    "def kmeansVisualWordsCreation(k, descriptor_list):\n",
    "    print(' . calculating central points for the existing feature values.')\n",
    "    #kmeansModel = KMeans(n_clusters = k, n_init=10)\n",
    "    batchSize = np.ceil(descriptor_list.__len__()/50).astype('int')\n",
    "    kmeansModel = MiniBatchKMeans(n_clusters=k, batch_size=batchSize, verbose=0)\n",
    "    kmeansModel.fit(descriptor_list)\n",
    "    visualWords = kmeansModel.cluster_centers_ # a.k.a. centers of reference\n",
    "    print(' . done calculating central points for the given feature set.')\n",
    "    return visualWords, kmeansModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa9817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the histograms. To create our each image by a histogram. We will create a vector of k values for each\n",
    "# image. For each keypoints in an image, we will find the nearest center, defined using training set\n",
    "# and increase by one its value\n",
    "\n",
    "\n",
    "def mapFeatureValsToHistogram (DataFeaturesByClass, visualWords, TrainedKmeansModel):\n",
    "    #depenting on the approach you may not need to use all inputs\n",
    "    histogramsList = []\n",
    "    targetClassList = []\n",
    "    numberOfBinsPerHistogram = visualWords.shape[0]\n",
    "    for categoryIdx, featureValues in DataFeaturesByClass.items():\n",
    "        for tmpImageFeatures in featureValues: #yes, we check one by one the values in each image for all images\n",
    "            tmpImageFeatures=tmpImageFeatures.astype(float)\n",
    "            tmpImageHistogram = np.zeros(numberOfBinsPerHistogram)\n",
    "            tmpIdx = list(TrainedKmeansModel.predict(tmpImageFeatures))\n",
    "            clustervalue, visualWordMatchCounts = np.unique(tmpIdx, return_counts=True)\n",
    "            tmpImageHistogram[clustervalue] = visualWordMatchCounts\n",
    "            # do not forget to normalize the histogram values\n",
    "            numberOfDetectedPointsInThisImage = tmpIdx.__len__()\n",
    "            tmpImageHistogram = tmpImageHistogram/numberOfDetectedPointsInThisImage\n",
    "\n",
    "            #now update the input and output coresponding lists\n",
    "            histogramsList.append(tmpImageHistogram)\n",
    "            targetClassList.append(categoryIdx)\n",
    "    return histogramsList, targetClassList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a747690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_of_input_image_classes = 'C:/Users/Chrispdl/Desktop/ml and natural language processing/hw2/archive (2)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f06ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 918 files [00:03, 294.23 files/s]\n"
     ]
    }
   ],
   "source": [
    "#here we run the code\n",
    "#define a fixed image size to work with\n",
    "inputImageSize = [200, 200, 3] #define the FIXED size that CNN will have as input\n",
    "\n",
    "import splitfolders\n",
    "splitfolders.ratio(path_of_input_image_classes, output=\"output\", seed=1, ratio=(0.8, 0,0.2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ecb76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the path to train and test files\n",
    "TrainImagesFilePath ='C:/Users/Chrispdl/Desktop/ml and natural language processing/hw2/output/train'\n",
    "TestImagesFilePath = 'C:/Users/Chrispdl/Desktop/ml and natural language processing/hw2/output/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da1de7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n"
     ]
    }
   ],
   "source": [
    "#load the train images\n",
    "trainImages = load_images_from_folder(TrainImagesFilePath, inputImageSize)  # take all images category by category for train set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708b8416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . start detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n"
     ]
    }
   ],
   "source": [
    "#calculate points and descriptor values per image\n",
    "trainDataFeatures = detector_features(trainImages)\n",
    "# Takes the descriptor list which is unordered one\n",
    "TrainDescriptorList = trainDataFeatures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a7217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . calculating central points for the existing feature values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=15\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . done calculating central points for the given feature set.\n"
     ]
    }
   ],
   "source": [
    "#create the central points for the histograms using k means.\n",
    "#here we use a rule of the thumb to create the expected number of cluster centers\n",
    "numberOfClasses = trainImages.__len__() #retrieve num of classes from dictionary\n",
    "possibleNumOfCentersToUse = 10 * numberOfClasses\n",
    "visualWords, TrainedKmeansModel = kmeansVisualWordsCreation(possibleNumOfCentersToUse, TrainDescriptorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2a78e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the sift feature values that is seperated class by class for train data, we need this to calculate the histograms\n",
    "trainBoVWFeatureVals = trainDataFeatures[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb52540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the train input train output format\n",
    "trainHistogramsList, trainTargetsList = mapFeatureValsToHistogram(trainBoVWFeatureVals, visualWords, TrainedKmeansModel)\n",
    "#X_train = np.asarray(trainHistogramsList)\n",
    "#X_train = np.concatenate(trainHistogramsList, axis=0)\n",
    "X_train = np.stack(trainHistogramsList, axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e49e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Categorical Data For Scikit-Learn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create a label (category) encoder object\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "labelEncoder.fit(trainTargetsList)\n",
    "#convert the categories from strings to names\n",
    "y_train = labelEncoder.transform(trainTargetsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43e5525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.43\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate the classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65504626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dddb7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on training set: 0.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "print('Accuracy of GNB classifier on training set: {:.2f}'.format(gnb.score(X_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3d1944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 0.82\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . Finished parsing images. What is next?\n",
      " . start detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      " . finished detecting points and calculating features for a given image set\n",
      "\n",
      " Printing performance scores:\n",
      "\n",
      "Accuracy scores of Decision Tree classifier are: train: 1.00 and test: 0.56.\n",
      "Precision scores of Decision Tree classifier are: train: 1.00 and test: 0.57.\n",
      "Recall scores of Decision Tree classifier are: train: 1.00 and test: 0.56.\n",
      "F1 scores of Decision Tree classifier are: train: 1.00 and test: 0.56.\n",
      "\n",
      "Accuracy scores of K-NN classifier are: train: 0.43 and test: 0.30.\n",
      "Precision scores of Logistic regression classifier are: train: 0.50 and test: 0.33.\n",
      "Recall scores of K-NN classifier are: train: 0.43 and test: 0.30.\n",
      "F1 scores of K-NN classifier are: train: 0.42 and test: 0.28.\n",
      "\n",
      "Accuracy scores of GNB classifier are: train: 0.39 and test: 0.28.\n",
      "Precision scores of GBN classifier are: train: 0.42 and test: 0.29.\n",
      "Recall scores of GNB classifier are: train: 0.39 and test: 0.27.\n",
      "F1 scores of GNB classifier are: train: 0.38 and test: 0.26.\n",
      "\n",
      "Accuracy scores of SVM classifier are: train: 0.82 and test: 0.56.\n",
      "Precision scores of SVM classifier are: train: 0.83 and test: 0.57.\n",
      "Recall scores of SVM classifier are: train: 0.82 and test: 0.56.\n",
      "F1 scores of SVM classifier are: train: 0.82 and test: 0.56.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'.format(svm.score(X_train, y_train)))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "#now run the same things on the test data.\n",
    "# DO NOT FORGET: you use the same visual words, created using training set.\n",
    "\n",
    "#clear some space\n",
    "del trainImages, trainBoVWFeatureVals, trainDataFeatures, TrainDescriptorList\n",
    "\n",
    "#load the train images\n",
    "testImages = load_images_from_folder(TestImagesFilePath, inputImageSize)  # take all images category by category for train set\n",
    "\n",
    "#calculate points and descriptor values per image\n",
    "testDataFeatures = detector_features(testImages)\n",
    "\n",
    "# Takes the sift feature values that is seperated class by class for train data, we need this to calculate the histograms\n",
    "testBoVWFeatureVals = testDataFeatures[1]\n",
    "\n",
    "#create the test input / test output format\n",
    "testHistogramsList, testTargetsList = mapFeatureValsToHistogram(testBoVWFeatureVals, visualWords, TrainedKmeansModel)\n",
    "X_test = np.array(testHistogramsList)\n",
    "y_test = labelEncoder.transform(testTargetsList)\n",
    "\n",
    "\n",
    "#classification tree\n",
    "# predict outcomes for test data and calculate the test scores\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "#calculate the scores\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "pre_train = precision_score(y_train, y_pred_train, average='macro')\n",
    "pre_test = precision_score(y_test, y_pred_test, average='macro')\n",
    "rec_train = recall_score(y_train, y_pred_train, average='macro')\n",
    "rec_test = recall_score(y_test, y_pred_test, average='macro')\n",
    "f1_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "#print the scores\n",
    "print('')\n",
    "print(' Printing performance scores:')\n",
    "print('')\n",
    "\n",
    "print('Accuracy scores of Decision Tree classifier are:',\n",
    "      'train: {:.2f}'.format(acc_train), 'and test: {:.2f}.'.format(acc_test))\n",
    "print('Precision scores of Decision Tree classifier are:',\n",
    "      'train: {:.2f}'.format(pre_train), 'and test: {:.2f}.'.format(pre_test))\n",
    "print('Recall scores of Decision Tree classifier are:',\n",
    "      'train: {:.2f}'.format(rec_train), 'and test: {:.2f}.'.format(rec_test))\n",
    "print('F1 scores of Decision Tree classifier are:',\n",
    "      'train: {:.2f}'.format(f1_train), 'and test: {:.2f}.'.format(f1_test))\n",
    "print('')\n",
    "\n",
    "# knn predictions\n",
    "#now check for both train and test data, how well the model learned the patterns\n",
    "y_pred_train = knn.predict(X_train)\n",
    "y_pred_test = knn.predict(X_test)\n",
    "#calculate the scores\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "pre_train = precision_score(y_train, y_pred_train, average='macro')\n",
    "pre_test = precision_score(y_test, y_pred_test, average='macro')\n",
    "rec_train = recall_score(y_train, y_pred_train, average='macro')\n",
    "rec_test = recall_score(y_test, y_pred_test, average='macro')\n",
    "f1_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "#print the scores\n",
    "print('Accuracy scores of K-NN classifier are:',\n",
    "      'train: {:.2f}'.format(acc_train), 'and test: {:.2f}.'.format(acc_test))\n",
    "print('Precision scores of Logistic regression classifier are:',\n",
    "      'train: {:.2f}'.format(pre_train), 'and test: {:.2f}.'.format(pre_test))\n",
    "print('Recall scores of K-NN classifier are:',\n",
    "      'train: {:.2f}'.format(rec_train), 'and test: {:.2f}.'.format(rec_test))\n",
    "print('F1 scores of K-NN classifier are:',\n",
    "      'train: {:.2f}'.format(f1_train), 'and test: {:.2f}.'.format(f1_test))\n",
    "print('')\n",
    "\n",
    "\n",
    "#naive Bayes\n",
    "# now check for both train and test data, how well the model learned the patterns\n",
    "y_pred_train = gnb.predict(X_train)\n",
    "y_pred_test = gnb.predict(X_test)\n",
    "# calculate the scores\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "pre_train = precision_score(y_train, y_pred_train, average='macro')\n",
    "pre_test = precision_score(y_test, y_pred_test, average='macro')\n",
    "rec_train = recall_score(y_train, y_pred_train, average='macro')\n",
    "rec_test = recall_score(y_test, y_pred_test, average='macro')\n",
    "f1_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "# print the scores\n",
    "print('Accuracy scores of GNB classifier are:',\n",
    "      'train: {:.2f}'.format(acc_train), 'and test: {:.2f}.'.format(acc_test))\n",
    "print('Precision scores of GBN classifier are:',\n",
    "      'train: {:.2f}'.format(pre_train), 'and test: {:.2f}.'.format(pre_test))\n",
    "print('Recall scores of GNB classifier are:',\n",
    "      'train: {:.2f}'.format(rec_train), 'and test: {:.2f}.'.format(rec_test))\n",
    "print('F1 scores of GNB classifier are:',\n",
    "      'train: {:.2f}'.format(f1_train), 'and test: {:.2f}.'.format(f1_test))\n",
    "print('')\n",
    "\n",
    "\n",
    "#support vector machines\n",
    "# now check for both train and test data, how well the model learned the patterns\n",
    "y_pred_train = svm.predict(X_train)\n",
    "y_pred_test = svm.predict(X_test)\n",
    "# calculate the scores\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "pre_train = precision_score(y_train, y_pred_train, average='macro')\n",
    "pre_test = precision_score(y_test, y_pred_test, average='macro')\n",
    "rec_train = recall_score(y_train, y_pred_train, average='macro')\n",
    "rec_test = recall_score(y_test, y_pred_test, average='macro')\n",
    "f1_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "# print the scores\n",
    "print('Accuracy scores of SVM classifier are:',\n",
    "      'train: {:.2f}'.format(acc_train), 'and test: {:.2f}.'.format(acc_test))\n",
    "print('Precision scores of SVM classifier are:',\n",
    "      'train: {:.2f}'.format(pre_train), 'and test: {:.2f}.'.format(pre_test))\n",
    "print('Recall scores of SVM classifier are:',\n",
    "      'train: {:.2f}'.format(rec_train), 'and test: {:.2f}.'.format(rec_test))\n",
    "print('F1 scores of SVM classifier are:',\n",
    "      'train: {:.2f}'.format(f1_train), 'and test: {:.2f}.'.format(f1_test))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c4dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d8f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364255fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b598811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
